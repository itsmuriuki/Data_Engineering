# Project 1: Data modelling with RDBMS (Postgres) 

## Summary
* [Introduction](#Introduction)
* [Project_Description](#Project_Description)
* [Schema definition](#Schema-definition)
* [How to run](#How-to-run)
* [Project structure](#Project-structure)
* [Example queries](#Example-queries)
--------------------------------------------


#### Introduction

A startup called Sparkify want to analyze the data they have been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to.

The aim is to create a Postgres Database Schema and ETL pipeline to optimize queries for song play analysis.

#### Project_Description

In this project, I have to model data with Postgres and build and ETL pipeline using Python. On the database side, I have to define fact and dimension tables for a Star Schema for a specific focus. On the other hand, ETL pipeline would transfer data from files located in two local directories into these tables in Postgres using Python and SQL

#### Data

There are 2 different types of Data that is available for the Sparkify music streaming application amd they are stored as JSON files. The following are the details regarding the same:

Song Files: It has all Songs, Albums and Artist related details. Here is one sample row: { "num_songs": 1, "artist_id": "ARD7TVE1187B99BFB1", "artist_latitude": null, "artist_longitude": null, "artist_location": "California - LA", "artist_name": "Casual", "song_id": "SOMZWCG12A8C13C480", "title": "I Didn't Mean To", "duration": 218.93179, "year": 0 }

Log Files: It has the logs of the user's music listening activity on the app. Here is one sample row: { "artist":null, "auth":"Logged In", "firstName":"Walter", "gender":"M", "itemInSession":0, "lastName":"Frye", "length":null, "level":"free", "location":"San Francisco-Oakland-Hayward, CA", "method":"GET", "page":"Home", "registration":1540919166796.0, "sessionId":38, "song":null, "status":200, "ts":1541105830796, "userAgent":""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36"","userId":"39" }

#### Schema definition
This is the schema of the database

How to read the schema: 
* Blank bullets are used to identify the fields that can be null <br>
* Black bullets are used to identify the fields that can not be null <br>
* If the field is underlined means that is a primary key <br>


![schema](./imgs/SongPlayAnalysisSchema.png)


To represent this context a ``Star schema`` has been used <br>

The songplays table is the core of this schema, is it our fact table and <br>
it contains foreign keys to four tables;
* start_time REFERENCES time(start_time)
* user_id REFERENCES time(start_time)
* song_id REFERENCES songs(song_id)
* artist_id REFERENCES artists(artist_id)


#### How to run

You need a PostgreSQL instance up and running <br>
Here you can find the [Binary packages](https://www.postgresql.org/download/) for you preferred operating system  <br>
After downloaded the package, If you do not know to move on just follow this [Tutorial](http://www.postgresqltutorial.com/), and python3 <br>


<b> Note: </b><br>
In this example we will use user-password authorization mechanism

After installing your database on your local machine, you have to create a <br>
custom user called `student` with password `student`  <br>
and create a database called `sparkifydb`

After opening terminal session, set your filesystem on project root folder <br>
and  insert these commands in order to run the demo: <br><br>
<I> This will create our tables, this must be ran first </I> <br>
`` python create_tables.py`` <br>

<I> And this will execute our ETL process </I> <br>
`` python etl.py`` <br>

#### Project structure
This is the project structure, if the bullet contains ``/`` <br>
means that the resource is a folder:

* <b> /data </b> - Source of the JSON file, all these files have to be elaborated
  * <b> /log_data </b> - A folder that contains files of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.
  * <b> /song_data </b> -  Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID
* <b> /imgs </b> - Simply a folder with images that are used in this ``md``
* <b> etl.ipynb </b> - It is a notebook that helps to know step by step what etl.py does
* <b> test.ipynb </b> - It is a notebook that helps to know if tables
  <br> are created and data are ingested correctly 
* <b> create_tables.py </b> - This script will drop old tables (if exist) ad re-create new tables
* <b> etl.py </b> - This script will read JSON every file contained in /data folder, parse them, <br> build relations though logical process and ingest data 
* <b> sql_queries.py </b> - This file contains variables with SQL statement in String formats, <br> partitioned by CREATE, DROP, INSERT statements plus a FIND query 


#### Example queries

<I> For strategical purposes you may want to know which is the most <br>
used user agent</I>
``` SQL
SELECT user_agent, count(user_agent) FROM songplays GROUP BY user_agent;
```

<I> For IT purposes you may want know the hour where users are more active </I>
``` SQL
SELECT count(start_time), DATE_PART('hour', start_time) from songplays group by DATE_PART('hour', start_time);
```
<I> Total number of users present in log table</I>
 
 %sql select count(distinct user_id) from songplays

    96

<I>songs played by Gender</I>

    %sql select a2.gender, count(a1.songplay_id) from songplays a1 join users a2 on a1.user_id = a2.user_id group by a2.gender

    M	1936

    F	4895
----------------------------